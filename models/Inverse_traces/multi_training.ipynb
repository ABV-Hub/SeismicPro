{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from seismicpro.batchflow import Pipeline, Dataset, B, V, D, C\n",
    "from seismicpro.batchflow.models.torch import ResNet18, VGG7\n",
    "from seismicpro.src import TraceIndex, SeismicDataset, FieldIndex, KNNIndex\n",
    "from seismicpro.src.seismic_batch import (SeismicBatch,\n",
    "                                            seismic_plot)\n",
    "from seismicpro.batchflow.models.torch.layers import ConvBlock\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismicpro.batchflow import action, inbatch_parallel\n",
    "\n",
    "class InverseBatch_2d(SeismicBatch):\n",
    "    @action\n",
    "    @inbatch_parallel(init='_init_component')\n",
    "    def inv_traces(self, index, src, dst, p=.5):\n",
    "        pos = self.get_pos(None, src, index)\n",
    "        traces = getattr(self, src)[pos]\n",
    "        size = traces.shape[0]\n",
    "        mask = np.random.choice([1, -1], size=size, p=(1-p, p))\n",
    "        getattr(self, dst[0])[pos] = traces * mask.reshape(-1, 1)\n",
    "        getattr(self, dst[1])[pos] = 1 - np.clip(mask, 0, 1)\n",
    "\n",
    "    @action\n",
    "    def update_batch(self, src, from_cont):\n",
    "        batch = getattr(self, src[0])\n",
    "        labels = getattr(self, src[1])\n",
    "        if from_cont[0] is None:\n",
    "            return self\n",
    "        new_data = from_cont[0][0]\n",
    "        new_labels = from_cont[0][1]\n",
    "        batch = np.vstack((batch, new_data))\n",
    "        labels = np.vstack((labels, new_labels))\n",
    "        setattr(self, src[0], batch)\n",
    "        setattr(self, src[1], labels)\n",
    "        return self\n",
    "\n",
    "    @action\n",
    "    def HNS(self, src, labels, preds, metric, to, n_worse=50):\n",
    "        order = metric(labels, preds)\n",
    "        sigm = torch.nn.Sigmoid()\n",
    "        preds = sigm(torch.Tensor(preds))\n",
    "        to[0] = [getattr(self, src)[order[:n_worse]], labels[order[:n_worse]], preds[order[:n_worse]]]\n",
    "        return self\n",
    "\n",
    "def draw_res(loss, scores, labels, title=' '):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(20, 9))\n",
    "    ax[0].plot(loss[-300:], label='Loss')\n",
    "    for i, score in enumerate(scores):\n",
    "        ax[1].plot(score, label=labels[i])\n",
    "    ax[0].set_title(title)\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    ax[0].grid()\n",
    "    ax[1].grid()\n",
    "    plt.show()\n",
    "    \n",
    "def create_test_ppl(train_ppl, data, mode='w'):\n",
    "    test_ppl = (Pipeline().load(components='raw', fmt='segy')\n",
    "              .standardize(src='raw', dst='raw')\n",
    "              .inv_traces(src='raw', dst=['raw', 'labels'], p=0.0026)\n",
    "              .import_model('model', train_ppl, 'model')\n",
    "              .preprocess_component(src='raw', dst='raw')\n",
    "              .preprocess_answers(src='labels', dst='labels')\n",
    "              .init_variable('pred', init_on_each_run=list())\n",
    "              .init_variable('labels', init_on_each_run=list())\n",
    "              .update_variable('labels', B('labels'), mode=mode) \n",
    "              .predict_model('model', B('raw'), fetches='predictions',\n",
    "                             save_to=V('pred', mode=mode))) << data\n",
    "    return test_ppl\n",
    "\n",
    "def get_results(ppl):\n",
    "    sigm = torch.nn.Sigmoid()\n",
    "    pred = sigm(torch.Tensor(ppl.v('pred')))\n",
    "    preds = np.array(np.array(pred) > .5, dtype=int).ravel()\n",
    "    labels = np.array(ppl.v('labels')).ravel()\n",
    "    return preds, labels\n",
    "\n",
    "def create_test_ppl(train_ppl, data, mode='w'):\n",
    "    test_ppl = (Pipeline().load(components='raw', fmt='segy')\n",
    "              .standardize(src='raw', dst='raw')\n",
    "              .inv_traces(src='raw', dst=['raw', 'labels'], p=0.0026)\n",
    "              .import_model('model', train_ppl, 'model')\n",
    "              .apply_transform_all(src='raw', dst='raw', func=lambda x: np.expand_dims(np.stack(x), axis=1).astype(np.float32))\n",
    "              .apply_transform_all(src='labels', dst='labels', func=lambda x: np.stack(x).astype(np.float32))\n",
    "              .init_variable('pred', init_on_each_run=list())\n",
    "              .init_variable('labels', init_on_each_run=list())\n",
    "              .update_variable('labels', B('labels'), mode=mode) \n",
    "              .predict_model('model', B('raw'), fetches='predictions',\n",
    "                             save_to=V('pred', mode=mode))) << data\n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGH = 2\n",
    "pal_path = '/data/FB/dataset_1/Pal_Flatiron_1k.sgy'\n",
    "wz_path = '/data/FB/dataset_2/WZ_Flatiron_1k.sgy'\n",
    "vor_path = '/data/FB/dataset_6/3_FBP_input_ffid_raw-500_off-800.sgy'\n",
    "\n",
    "pal_index = KNNIndex(name='raw', path=pal_path, extra_headers=['offset'], n_neighbors=N_NEIGH)\n",
    "pal_index = pal_index.create_subset(pal_index.indices[:100000])\n",
    "pal_index.split()\n",
    "pal_data_tr = Dataset(pal_index.train, InverseBatch_2d)\n",
    "pal_data_te = Dataset(pal_index.test, InverseBatch_2d)\n",
    "\n",
    "wz_index = KNNIndex(name='raw', path=wz_path, extra_headers=['offset'], n_neighbors=N_NEIGH)\n",
    "wz_index = wz_index.create_subset(wz_index.indices[:100000])\n",
    "wz_index.split()\n",
    "wz_data_tr = Dataset(wz_index.train, InverseBatch_2d)\n",
    "wz_data_te = Dataset(wz_index.test, InverseBatch_2d)\n",
    "\n",
    "vor_index = KNNIndex(name='raw', path=vor_path, extra_headers=['offset'], n_neighbors=N_NEIGH)\n",
    "vor_index = vor_index.create_subset(vor_index.indices[:100000])\n",
    "vor_index.split()\n",
    "vor_data_tr = Dataset(vor_index.train, InverseBatch_2d)\n",
    "vor_data_te = Dataset(vor_index.test, InverseBatch_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsts_tr = np.array([pal_data_tr, wz_data_tr, vor_data_tr])\n",
    "dsts_te = np.array([pal_data_te, wz_data_te, vor_data_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismicpro.batchflow import L, I\n",
    "prep_ppl = (Pipeline()\n",
    "          .load(components='raw', fmt='segy', tslice=np.arange(751))\n",
    "          .standardize(src='raw', dst='raw')\n",
    "          .init_variable('prob', init_on_each_run=float)\n",
    "          .init_variable('n_iter', init_on_each_run=0)\n",
    "          .update_variable('prob', L(lambda x, m: m/(2*m + x**(1.4)))(V('n_iter'), C('n_iter')), mode='w')\n",
    "          .inv_traces(src='raw', dst=['raw', 'labels'], p=V('prob'))\n",
    "          .update_variable('n_iter', V('n_iter')+1)\n",
    "          .apply_transform_all(src='raw', dst='raw', func=lambda x: np.expand_dims(np.stack(x), axis=1).astype(np.float32))\n",
    "          .apply_transform_all(src='labels', dst='labels', func=lambda x: np.stack(x).astype(np.float32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 751\n",
    "inputs_config = {\n",
    "    'raw': {'shape': (1, N_NEIGH, SIZE)}, \n",
    "    'masks': {'shape': (N_NEIGH, )}\n",
    "    }\n",
    "\n",
    "config = {\n",
    "    'loss': 'bce',\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'raw',\n",
    "    'optimizer': 'Adam',\n",
    "    'head' : dict(layout='Vf', utils=N_NEIGH),\n",
    "    'n_iters': D('size')/B('size'),\n",
    "    'decay': dict(name='exp', gamma=0.99),\n",
    "    'device': 'gpu:0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(labels, preds):\n",
    "    sigm = torch.nn.Sigmoid()\n",
    "    pred = sigm(torch.Tensor(preds))\n",
    "    preds = np.array(np.array(pred) > .5, dtype=int).ravel()\n",
    "    labels = np.array(labels).ravel()\n",
    "    argsort = np.argsort(preds != labels)[::-1] / N_NEIGH\n",
    "    return argsort.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_init\n"
     ]
    }
   ],
   "source": [
    "load_config = {'load' : {'path' : './decrease_prob'},\n",
    "                                    'build': False,\n",
    "                                    'device': 'gpu:0'}\n",
    "\n",
    "batches = np.array([None])\n",
    "train_ppl = prep_ppl + (Pipeline()\n",
    "             .init_model('dynamic', ResNet18, 'model', config)\n",
    "             .update_batch(['raw', 'labels'], from_cont=batches)\n",
    "             .init_variable('loss', init_on_each_run=list)\n",
    "             .init_variable('pred', init_on_each_run=list)\n",
    "             .init_variable('labels', init_on_each_run=list)\n",
    "             .update_variable('labels', B('labels'), mode='w')\n",
    "             .train_model('model', B('raw'), B('labels'),  \n",
    "                          fetches=['loss', 'predictions'], save_to=[V('loss', mode='a'), V('pred', mode='w')])\n",
    "             .HNS('raw', V('labels'), V('pred'), n_worse=50, metric=metric, to=batches)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = [list(combinations([1, 2, 3], i)) for i in range(1, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comb = []\n",
    "for i in comb:\n",
    "    for j in i:\n",
    "        new_comb.append(np.array(j)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/151 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 151\n",
    "N_ITER = 200\n",
    "B_SIZE = 400\n",
    "ppl_config = dict(n_iter=N_ITER)\n",
    "global_scores = []\n",
    "\n",
    "for ixs in tqdm(new_comb):\n",
    "    train_numb = dsts_tr[ixs]\n",
    "    if not isinstance(train_numb, np.ndarray):\n",
    "        train_numb = np.array([train_numb])\n",
    "    ppl_train = (train_ppl << ppl_config)\n",
    "    ppl_train = ppl_train.run_later(B_SIZE, shuffle=True, drop_last=True)\n",
    "    ppl_train._init_all_variables()#reset('vars')\n",
    "    ppl_val = [create_test_ppl(ppl_train, data, mode='a') for data in dsts_te]\n",
    "    scores = [[] for _ in range(len(ppl_val))]\n",
    "\n",
    "    for i in tqdm(range(N_EPOCHS)):\n",
    "        for iters in range(N_ITER):\n",
    "            for dst in train_numb:\n",
    "                batch = dst.next_batch(B_SIZE, shuffle=True, drop_last=True)\n",
    "                ppl_train.execute_for(batch)\n",
    "        if i % 1 == 0:\n",
    "            print(i)\n",
    "            ppl_train.save_model_now('model', './models/{}/model_{}_ep_{}'.format(ixs, ixs, i))\n",
    "        for i, ppl_v in enumerate(ppl_val):\n",
    "            ppl_v.run(B_SIZE, shuffle=True, drop_last=True, n_iters=1, reset=['iter', 'vars'])##change!\n",
    "            preds, labels = get_results(ppl_v)\n",
    "            scores[i].append(f1_score(labels, preds))\n",
    "        clear_output()\n",
    "        draw_res(ppl_train.v('loss'), scores, ['pal', 'wz', 'vor'])\n",
    "        ppl_train.save_model_now('model', './models/{}/model_{}_ep_{}'.format(ixs, ixs, i))\n",
    "    global_scores.append(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
