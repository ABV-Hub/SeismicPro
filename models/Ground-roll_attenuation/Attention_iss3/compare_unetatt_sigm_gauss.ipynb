{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from seismicpro.batchflow import Pipeline, V, B, L, I, W, C\n",
    "# from seismicpro.batchflow.models.tf.layers import conv_block\n",
    "# from seismicpro.batchflow.models.tf import UNet\n",
    "\n",
    "from seismicpro.src import (SeismicDataset, FieldIndex, TraceIndex,\n",
    "                           # , statistics_plot,\n",
    "#                            seismic_plot, spectrum_plot, \n",
    "                            merge_segy_files\n",
    "                           )\n",
    "from seismicpro.models import UnetAtt, UnetAttGauss1, attention_loss, attention_loss_gauss, FieldMetrics\n",
    "\n",
    "\n",
    "from seismicpro.batchflow.batchflow.research import Research, Option, KV\n",
    "from seismicpro.batchflow.batchflow.utils import plot_results_by_config\n",
    "\n",
    "\n",
    "from metric_utils import get_windowed_spectrogram_dists\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'initial_block/inputs': 'trace_raw',\n",
    "    'inputs': dict(trace_raw={'shape': (3000, 1)},\n",
    "                   lift={'name': 'targets', 'shape': (3000, 1)}),\n",
    "\n",
    "#     'loss': (C('loss'), {'balance': 0.05}),\n",
    "    'optimizer': ('Adam', {'learning_rate': 0.0001}),\n",
    "    'common/data_format': 'channels_last',\n",
    "    'body': {\n",
    "        'main': {\n",
    "            'encoder/num_stages': 5,\n",
    "            'encoder/blocks': dict(layout='ca ca',\n",
    "                                   filters=[16, 32, 64, 128, 256],\n",
    "                                   kernel_size=[7, 5, 5, 5, 5],\n",
    "                                   activation=tf.nn.elu),\n",
    "            'encoder/downsample': dict(layout='pd',\n",
    "                                       pool_size=2,\n",
    "                                       pool_strides=2,\n",
    "                                       dropout_rate=0.05),\n",
    "\n",
    "            'decoder/blocks': dict(layout='ca ca',\n",
    "                                   filters=[16, 32, 64, 128, 256][::-1],\n",
    "                                   kernel_size=[7, 5, 5, 5, 5][::-1],\n",
    "                                   activation=tf.nn.elu),\n",
    "            'decoder/upsample': dict(layout='tad',\n",
    "                                     kernel_size=[7, 5, 5, 5, 5][::-1],\n",
    "                                     strides=2,\n",
    "                                     dropout_rate=0.05,\n",
    "                                     activation=tf.nn.elu,),\n",
    "          },\n",
    "        'attn': {\n",
    "              'encoder/num_stages': 5,\n",
    "              'encoder/blocks': dict(layout='ca ca',\n",
    "                                     filters=[8, 16, 32, 64, 128],\n",
    "                                     kernel_size=3,\n",
    "                                     activation=tf.nn.elu),\n",
    "              'encoder/downsample': dict(layout='pd',\n",
    "                                         pool_size=2,\n",
    "                                         pool_strides=2,\n",
    "                                         dropout_rate=0.05),\n",
    "\n",
    "              'decoder/blocks': dict(layout='ca ca',\n",
    "                                     filters=[8, 16, 32, 64, 128][::-1],\n",
    "                                     kernel_size=3,\n",
    "                                     activation=tf.nn.elu),\n",
    "              'decoder/upsample': dict(layout='ta d',\n",
    "                                       kernel_size=3,\n",
    "                                       strides=2,\n",
    "                                       dropout_rate=0.05,\n",
    "                                       activation=tf.nn.elu),\n",
    "        },\n",
    "    },\n",
    "    'head': {'scale': 1.5},\n",
    "    'train_steps': {\n",
    "        'step_main': {'scope': 'main_branch'},\n",
    "        'step_attention': {'scope': 'attention_branch'},\n",
    "\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "def exp_stack(x):\n",
    "    return np.expand_dims(np.vstack(x), -1)\n",
    "\n",
    "def make_data(batch, **kwagrs):\n",
    "    return {'trace_raw': exp_stack(batch.raw), 'lift': exp_stack(batch.lift)}\n",
    "\n",
    "def make_data_inference(batch, **kwagrs):\n",
    "    return {'trace_raw': exp_stack(batch.raw)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_save(model, loss, train_set, model_path='./saved_models', **kwargs):\n",
    "\n",
    "    fi = train_set.indices\n",
    "    \n",
    "    tindex = TraceIndex(train_set.index)\n",
    "    t_train_set = SeismicDataset(tindex)\n",
    "    \n",
    "    train_pipeline = (t_train_set.p\n",
    "                      .load(components=('raw', 'lift'), fmt='segy', tslice=np.arange(3000))\n",
    "                      .init_model('dynamic', model, name='unet', \n",
    "                                  config={**model_config, **{'loss': (loss, {'balance': 0.05})}})\n",
    "                      .init_variable('loss', default=list())\n",
    "                      .train_model('unet', make_data=make_data, fetches='loss_step_main', save_to=V('loss', 'a'))\n",
    "                     )\n",
    "    \n",
    "    batch_size = kwargs.get('batch_size', 64)\n",
    "    \n",
    "    if 'n_epochs' not in kwargs and 'n_iters' not in kwargs:\n",
    "        n_epochs = 1\n",
    "        n_iters = None\n",
    "    else:\n",
    "        n_epochs = kwargs.get('n_epochs', None)\n",
    "        n_iters = kwargs.get('n_iters', None)\n",
    "    \n",
    "    train_pipeline = train_pipeline.run(batch_size=batch_size, n_epochs=n_epochs, n_iters=n_iters, drop_last=True,\n",
    "                                    shuffle=True, bar=True, bar_desc=W(V('loss')[-1].format('Current loss is: {:7.7}')))\n",
    "    \n",
    "#     tz = datetime.timezone(datetime.timedelta(hours=3))\n",
    "#     path = os.path.join(model_path, str(datetime.datetime.now(tz=tz)).replace(' ', '_'))\n",
    "#     print(path)\n",
    "    path = model_path\n",
    "\n",
    "    train_pipeline.save_model_now('unet', path)\n",
    "    \n",
    "    loss = np.array(train_pipeline.get_variable('loss'))\n",
    "    \n",
    "    readme = os.path.join(path, 'README.txt')\n",
    "    with open(readme, 'w') as inpf:\n",
    "        inpf.write(\"Model name: {}\\n\".format(model.__name__))\n",
    "        inpf.write(\"Avg final loss (100 points): {}\\n\".format(np.mean(loss[-100:])))\n",
    "        inpf.write(\"\\nConfig:\\n\")\n",
    "        inpf.write(pprint.pformat(model_config, compact=True))\n",
    "        inpf.write(\"\\n\\nAdditional Info:\\n\")\n",
    "        inpf.write(pprint.pformat(dict(fields=list(fi), **kwargs)))\n",
    "        \n",
    "    return loss, path, fi\n",
    "\n",
    "\n",
    "def inference(model, model_path, test_set, output_path=None, tmp_dump_path='tmp', clear=False):\n",
    "    if os.path.exists(tmp_dump_path):\n",
    "        shutil.rmtree(tmp_dump_path)\n",
    "    \n",
    "    os.makedirs(tmp_dump_path)\n",
    "    \n",
    "    tindex = TraceIndex(test_set.index)\n",
    "    t_test_set = SeismicDataset(tindex)\n",
    "    \n",
    "    inference_ppl = (t_test_set.p\n",
    "                     .load_model(\"dynamic\", model, 'unet', path=model_path)\n",
    "                     .init_variable('res')\n",
    "                     .load(components='raw', fmt='segy', tslice=np.arange(3000))\n",
    "                     .predict_model('unet', make_data=make_data_inference,\n",
    "                                    fetches=['out_lift'], save_to=B('raw'))\n",
    "                     .dump(path=L(lambda x: os.path.join(tmp_dump_path, str(x) + '.sgy'))(I()),\n",
    "                           src='raw', fmt='segy', split=False)\n",
    "                 )\n",
    "    inference_ppl.run(1000, n_epochs=1, drop_last=False, shuffle=False, bar=True)\n",
    "    \n",
    "    if output_path is None:\n",
    "        clear = False\n",
    "        output_path = os.path.join(tmp_dump_path, 'merged.sgy')\n",
    "        \n",
    "    print(\"merging .sgy\")\n",
    "    merge_segy_files(output_path=output_path, extra_headers='all', path=os.path.join(tmp_dump_path, '*.sgy'))\n",
    "    \n",
    "    if clear:\n",
    "        if os.path.exists(tmp_dump_path):\n",
    "            shutil.rmtree(tmp_dump_path)        \n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def eval_mt(batch, *args):\n",
    "    mt = FieldMetrics(batch.lift[0], batch.ml[0])\n",
    "    return mt.mae(), mt.corr_coef()\n",
    "\n",
    "def eval_dist(batch, *args):\n",
    "    n_use_traces = 200\n",
    "    dist_m = get_windowed_spectrogram_dists(batch.lift[0][0:n_use_traces], batch.ml[0][0:n_use_traces])\n",
    "    dist = np.mean(dist_m)\n",
    "    return dist\n",
    "\n",
    "def _test(path_lift, model_out, fi):\n",
    "    m_index = (FieldIndex(name='ml', path=model_out)\n",
    "               .merge(FieldIndex(name='lift', path=path_lift)))\n",
    "    \n",
    "    dset = SeismicDataset(m_index)   \n",
    "    metr_pipeline = (Pipeline()\n",
    "                 .init_variable('mt', default=[])\n",
    "                 .init_variable('dist', default=[])\n",
    "                 .load(components=('ml', 'lift'), fmt='segy', tslice=np.arange(3000))\n",
    "                 .call(eval_mt, save_to=V('mt', mode='a'))\n",
    "                 .call(eval_dist, save_to=V('dist', mode='a')))\n",
    "    \n",
    "    metr_pipeline = metr_pipeline << dset\n",
    "    metr_pipeline = metr_pipeline.run(batch_size=1, n_epochs=1, drop_last=False,\n",
    "                                      shuffle=False, bar=True)\n",
    "    mt = np.vstack(metr_pipeline.get_variable('mt'))\n",
    "    dist = np.asarray(metr_pipeline.get_variable('dist'))\n",
    "    \n",
    "    return np.mean(mt[:, 0]), np.mean(mt[:, 1]), np.mean(dist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loss_fn, splits, dataset_name, path_lift):\n",
    "    losses = []\n",
    "    save_res = []\n",
    "    for i, split in enumerate(splits): # [ds.cv0, ds.cv1, ds.cv2, ds.cv3, ds.cv4]):\n",
    "        print(\"Processing dataset {}, model {}, cv_split {}\".format(dataset_name, model.__name__, i))\n",
    "        # NB split.test is passed as train and vice versa\n",
    "        loss, model_path, fi = train_n_save(model, loss_fn, split.test, model_path='./saved_models/{}/{}/{}'.format(model.__name__, dataset_name, i), n_epochs=3)\n",
    "        losses.append((model.__name__, dataset_name, i, loss))\n",
    "        print(\"mean final loss:\", np.mean(loss[-100:]))\n",
    "\n",
    "        output_path = inference(model, model_path, split.train, output_path=os.path.join(model_path, '{}_out.sgy'.format(dataset_name)), clear=True)\n",
    "        print(output_path)\n",
    "\n",
    "        mae, corr, dist = _test(path_lift, output_path, fi)\n",
    "        print(\"mae, corr, dist\")\n",
    "        print(mae, corr, dist)\n",
    "\n",
    "        save_res.append([model.__name__, dataset_name, i, mae, corr, dist])\n",
    "        \n",
    "    return losses, save_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'H1_WZ'\n",
    "path_raw = '/notebooks/data/H1_WZ/1_NA-gr_input_DN01_norm2.sgy'\n",
    "path_lift = '/notebooks/data/H1_WZ/1_NA-gr_output_DN03_norm2.sgy'\n",
    "\n",
    "index = (FieldIndex(name='raw', extra_headers=['offset'], path=path_raw)\n",
    "             .merge(FieldIndex(name='lift', path=path_lift)))\n",
    "    \n",
    "findex = FieldIndex(index)\n",
    "ds = SeismicDataset(findex)\n",
    "ds.cv_split()\n",
    "splits = [ds.cv0, ds.cv1, ds.cv2, ds.cv3, ds.cv4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1707 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset H1_WZ, model UnetAtt, cv_split 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss is: 0.08191239:  20%|█▉        | 338/1707 [16:58<1:08:04,  2.98s/it]"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "save_res = []\n",
    "\n",
    "for model, loss in zip([UnetAtt, UnetAttGauss1], [attention_loss, attention_loss_gauss]):\n",
    "    l, r = process(model, loss, splits, dataset_name, path_lift)\n",
    "    losses.extend(l)\n",
    "    save_res.extend(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, dataset_name, i, loss in losses:\n",
    "    plt.plot(loss, label=model+\"_{} {}\".format(i, np.mean(loss[-100:])))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res = pd.DataFrame.from_records(save_res, columns=['Model', 'DS_train', 'split_no', 'MAE', 'Corr', 'Dist'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf14_kernel",
   "language": "python",
   "name": "tf14_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
