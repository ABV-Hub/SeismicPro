{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research that studies depth of main and attention branches in unet_att_params model, that reconstructs noise in main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import shutil\n",
    "\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from seismicpro.batchflow import Pipeline, C, V, B, L, I, W\n",
    "\n",
    "from seismicpro.batchflow.batchflow.models.tf import UNet, VGG7, VGG16, VGG19, ResNet, VGG\n",
    "from seismicpro.batchflow.batchflow.models.tf.nn.activations import h_sigmoid\n",
    "\n",
    "from seismicpro.batchflow.batchflow.research import Research, Option, KV, Results\n",
    "from seismicpro.batchflow.batchflow.utils import plot_results_by_config\n",
    "\n",
    "from seismicpro.src import (SeismicDataset, FieldIndex, TraceIndex, statistics_plot,\n",
    "                            seismic_plot, spectrum_plot, merge_segy_files)\n",
    "from seismicpro.models import attention_loss_gauss, attention_loss, UnetAtt\n",
    "from seismicpro.models.unetatt_3 import UnetAttention4, UnetAttention3\n",
    "\n",
    "\n",
    "from Ground_Roll.src.unet_att2_params import UnetAttParams\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "GPUs=[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>TraceNumber</th>\n",
       "      <th>offset</th>\n",
       "      <th>TRACE_SEQUENCE_FILE</th>\n",
       "      <th>file_id</th>\n",
       "      <th>TRACE_SEQUENCE_FILE</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>raw</th>\n",
       "      <th>lift</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FieldRecord</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111906</th>\n",
       "      <td>1512</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy</td>\n",
       "      <td>1</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111906</th>\n",
       "      <td>1513</td>\n",
       "      <td>326</td>\n",
       "      <td>2</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy</td>\n",
       "      <td>2</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111906</th>\n",
       "      <td>1511</td>\n",
       "      <td>333</td>\n",
       "      <td>3</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy</td>\n",
       "      <td>3</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111906</th>\n",
       "      <td>1514</td>\n",
       "      <td>334</td>\n",
       "      <td>4</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy</td>\n",
       "      <td>4</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111906</th>\n",
       "      <td>1515</td>\n",
       "      <td>348</td>\n",
       "      <td>5</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy</td>\n",
       "      <td>5</td>\n",
       "      <td>/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TraceNumber offset TRACE_SEQUENCE_FILE  \\\n",
       "                                               raw   \n",
       "FieldRecord                                          \n",
       "111906             1512    326                   1   \n",
       "111906             1513    326                   2   \n",
       "111906             1511    333                   3   \n",
       "111906             1514    334                   4   \n",
       "111906             1515    348                   5   \n",
       "\n",
       "                                                     file_id  \\\n",
       "                                                         raw   \n",
       "FieldRecord                                                    \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy   \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy   \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy   \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy   \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy   \n",
       "\n",
       "            TRACE_SEQUENCE_FILE  \\\n",
       "                           lift   \n",
       "FieldRecord                       \n",
       "111906                        1   \n",
       "111906                        2   \n",
       "111906                        3   \n",
       "111906                        4   \n",
       "111906                        5   \n",
       "\n",
       "                                                      file_id  \n",
       "                                                         lift  \n",
       "FieldRecord                                                    \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy  \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy  \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy  \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy  \n",
       "111906       /notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_raw = '/notebooks/data/H1_WZ/NA/1_NA-gr_input_DN01.sgy',\n",
    "path_lift = '/notebooks/data/H1_WZ/NA/1_NA-gr_output_DN03.sgy',\n",
    "\n",
    "index = (FieldIndex(name='raw', extra_headers=['offset'], path=path_raw)\n",
    "         .merge(FieldIndex(name='lift', path=path_lift)))\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fields: 101\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of fields:\", len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in CV: [111906, 111907, 111910, 111912, 111914, 111919, 111920, 111921, 111922, 111923, 111925, 111930, 111932, 111933, 111934, 111935, 111937, 111938, 111940, 111942, 111943, 111944, 111946, 111948, 111950, 111952, 111953, 111954, 111956, 111958, 111960, 111961, 111962, 111963, 111964, 111966, 111968, 111971, 111975, 111976, 111978, 111979, 111980, 111984, 111985, 111986, 111991, 111992, 111993, 111994, 111995, 111996, 111999, 112000, 112003, 112004, 112006, 112007, 112008, 112009]\n",
      "Fields not in CV: [111908, 111909, 111911, 111913, 111915, 111916, 111917, 111918, 111924, 111927, 111928, 111929, 111931, 111936, 111939, 111941, 111945, 111951, 111955, 111957, 111959, 111965, 111967, 111969, 111970, 111972, 111973, 111974, 111977, 111981, 111982, 111983, 111987, 111988, 111989, 111990, 111997, 111998, 112001, 112002, 112005]\n"
     ]
    }
   ],
   "source": [
    "tri = np.random.choice(len(index.indices), size=60, replace=False)\n",
    "train_indices = index.indices[tri]\n",
    "\n",
    "print(\"Fields in CV:\", sorted(train_indices))\n",
    "print(\"Fields not in CV:\", sorted(set(index.indices) - set(train_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traces in train set: 95616\n"
     ]
    }
   ],
   "source": [
    "tindex = TraceIndex(index.create_subset(train_indices))\n",
    "train_set = SeismicDataset(tindex)\n",
    "print(\"Number of traces in train set:\", len(tindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_config = {\n",
    "    'initial_block/inputs': 'trace_raw',\n",
    "    'inputs': dict(trace_raw={'shape': (3000, 1)},\n",
    "                   lift={'name': 'targets', 'shape': (3000, 1)}),\n",
    "    'optimizer': ('Adam', {'learning_rate': 0.0001}),\n",
    "    'common/data_format': 'channels_last',\n",
    "}\n",
    "\n",
    "main_bl_config = {\n",
    "    'encoder/num_stages': 4,\n",
    "    'encoder/blocks': dict(layout='caca', filters=[16, 32, 64, 128], kernel_size=7, activation=tf.nn.elu),\n",
    "    'encoder/downsample': dict(layout='pd', pool_size=2, pool_strides=2, dropout_rate=0.05),\n",
    "    'encoder/order': ['block', 'skip', 'downsampling'],\n",
    "    'embedding': dict(layout='caca', kernel_size=7, filters=256),\n",
    "    'decoder/num_stages': 4,\n",
    "    'decoder/blocks': dict(layout='caca', filters=[16, 32, 64, 128][::-1],\n",
    "                           kernel_size=7, activation=tf.nn.elu),\n",
    "    'decoder/upsample': dict(layout='tad', kernel_size=7, strides=2,\n",
    "                             filters=[16, 32, 64, 128][::-1],\n",
    "                             dropout_rate=0.05, activation=tf.nn.elu,),\n",
    "    'decoder/order': ['upsampling', 'combine', 'block'],\n",
    "}\n",
    "\n",
    "unetatt_old_bl_config = {\n",
    "    'loss': (attention_loss, {'balance': 0.05}),\n",
    "    'body': {'main': main_bl_config,\n",
    "    'attn': {\n",
    "                'encoder/num_stages': 3,\n",
    "                'encoder/blocks': dict(layout='caca', filters=[8, 16, 32], kernel_size=3, activation=tf.nn.elu),\n",
    "                'encoder/downsample': dict(layout='pd', pool_size=2, pool_strides=2, dropout_rate=0.05),\n",
    "                'embedding': dict(layout='caca', filters=64, kernel_size=3, activation=tf.nn.elu),\n",
    "                'decoder/num_stages': 3,\n",
    "                'decoder/blocks': dict(layout='caca', filters=[8, 16, 32][::-1], kernel_size=3, activation=tf.nn.elu),\n",
    "                'decoder/upsample': dict(layout='tad', filters=[8, 16, 32][::-1], kernel_size=3, strides=2,\n",
    "                                 dropout_rate=0.05, activation=tf.nn.elu),\n",
    "            },\n",
    "    },\n",
    "    'train_steps': {\n",
    "        'step_main': {'scope': 'main_branch'},\n",
    "        'step_attention': {'scope': ['attention_branch', 'attention_dense']},\n",
    "    }\n",
    "}\n",
    "\n",
    "main_config_small = {\n",
    "    'encoder/num_stages': 4,\n",
    "    'encoder/blocks': dict(layout='caca', \n",
    "                           filters=[16, 32, 64, 128], \n",
    "                           kernel_size=[7, 5, 5, 5], \n",
    "                           activation=tf.nn.elu),\n",
    "    'encoder/downsample': dict(layout='pd', pool_size=2, pool_strides=2, dropout_rate=0.05),\n",
    "    'encoder/order': ['block', 'skip', 'downsampling'],\n",
    "    'embedding': dict(layout='ca ca', kernel_size=5, filters=256),\n",
    "    'decoder/num_stages': 4,\n",
    "    'decoder/blocks': dict(layout='caca', \n",
    "                           filters=[16, 32, 64, 128][::-1],\n",
    "                           kernel_size=[7, 5, 5, 5][::-1], \n",
    "                           activation=tf.nn.elu),\n",
    "    'decoder/upsample': dict(layout='tad',\n",
    "                             filters=[16, 32, 64, 128][::-1],\n",
    "                             kernel_size=[7, 5, 5, 5][::-1], \n",
    "                             strides=2, dropout_rate=0.05, activation=tf.nn.elu,),\n",
    "    'decoder/order': ['upsampling', 'combine', 'block'],\n",
    "}\n",
    "\n",
    "unetatt_new_small_config = {\n",
    "    'loss': (attention_loss, {'balance': 0.05}),\n",
    "    'body' : main_config_small,\n",
    "    'body/attention': {'blocks': dict(layout='caca', filters=[128, 64, 32, 16], kernel_size=5, activation=tf.nn.elu),\n",
    "                       'upsample': dict(layout='tad', filters=[128, 64, 32, 16], kernel_size=5, strides=2,\n",
    "                                        dropout_rate=0.05, activation=tf.nn.elu)},\n",
    "}\n",
    "\n",
    "unetatt2_new_small_config = {\n",
    "    **unetatt_new_small_config,\n",
    "    'body/attention/skip': True\n",
    "}\n",
    "\n",
    "unetatt_new_small_for4_config = {\n",
    "    **unetatt_new_small_config,\n",
    "    'body/attention': {'blocks': dict(layout='caca', filters=[128, 64, 32, 16], kernel_size=[3, 5, 5, 5], activation=tf.nn.elu),\n",
    "                       'upsample': dict(layout='tad', filters=[128, 64, 32, 16], kernel_size=[3, 5, 5, 5], strides=2,\n",
    "                                        dropout_rate=0.05, activation=tf.nn.elu)},\n",
    "}\n",
    "\n",
    "m1_vgg_small = {\n",
    "    'loss': (attention_loss_gauss, {'balance': 0.05}),\n",
    "    'common/main_base_class': UNet,\n",
    "    'common/att_base_class': VGG,\n",
    "    'body': {\n",
    "        'main': main_config_small,\n",
    "        'att': {'arch': [\n",
    "                                (3, 0, 16, 1),\n",
    "                                (3, 0, 32, 1),\n",
    "                                (3, 0, 64, 1),\n",
    "                                (3, 0, 128, 1),\n",
    "                                (2, 0, 256, 1),\n",
    "                            ]\n",
    "            },\n",
    "    },\n",
    "    'head': {\n",
    "        'main': dict(layout='c', filters=1, units=1),\n",
    "        'att': dict(layout='fa', units=2, activation=h_sigmoid),\n",
    "    },\n",
    "    \n",
    "    'train_steps': {'step_main': {'scope': 'main_branch'},\n",
    "                 'step_attention': {'scope': 'attention_branch'}}\n",
    "}\n",
    "\n",
    "m1_resnet_small = {\n",
    "    **m1_vgg_small,\n",
    "    'common/att_base_class': ResNet,\n",
    "    'body/att': {'num_blocks': [2, 2, 2, 2], 'filters': [16, 32, 64, 128], 'kernel_size': 5},\n",
    "}\n",
    "\n",
    "m3_vgg = {\n",
    "    **m1_vgg_small,\n",
    "    'head/mode': 'noise'\n",
    "}\n",
    "\n",
    "m3_resnet = {\n",
    "    **m1_resnet_small,\n",
    "    'head/mode': 'noise'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ConfigAlias({'model': 'UnetAtt', 'loss_name': 'loss_step_main', 'model_config': 'egor_baseline'}), ConfigAlias({'model': 'UnetAttention3', 'loss_name': 'loss', 'model_config': 'unetatt2_small'}), ConfigAlias({'model': 'UnetAttention3', 'loss_name': 'loss', 'model_config': 'unetatt3_small'}), ConfigAlias({'model': 'UnetAttention4', 'loss_name': 'loss', 'model_config': 'unetatt4_small'}), ConfigAlias({'model': 'UnetAttParams', 'loss_name': 'loss_step_main', 'model_config': 'm1_vgg_small'}), ConfigAlias({'model': 'UnetAttParams', 'loss_name': 'loss_step_main', 'model_config': 'm1_resnet_small'}), ConfigAlias({'model': 'UnetAttParams', 'loss_name': 'loss_step_main', 'model_config': 'm3_vgg_small'}), ConfigAlias({'model': 'UnetAttParams', 'loss_name': 'loss_step_main', 'model_config': 'm3_resnet_small'})]\n"
     ]
    }
   ],
   "source": [
    "items = (\n",
    "    ('egor_baseline', UnetAtt, {**common_config, **unetatt_old_bl_config}, 'loss_step_main'),\n",
    "    ('unetatt2_small', UnetAttention3, {**common_config, **unetatt2_new_small_config}, 'loss'),\n",
    "    ('unetatt3_small', UnetAttention3, {**common_config, **unetatt_new_small_config}, 'loss'),\n",
    "    ('unetatt4_small', UnetAttention4, {**common_config, **unetatt_new_small_for4_config}, 'loss'),\n",
    "    ('m1_vgg_small', UnetAttParams, {**common_config, **m1_vgg_small}, 'loss_step_main'),  \n",
    "    ('m1_resnet_small', UnetAttParams, {**common_config, **m1_resnet_small}, 'loss_step_main'),\n",
    "    ('m3_vgg_small', UnetAttParams, {**common_config, **m3_vgg}, 'loss_step_main'),  \n",
    "    ('m3_resnet_small', UnetAttParams, {**common_config, **m3_resnet}, 'loss_step_main'),\n",
    ")\n",
    "\n",
    "keys, models, configs, losses = tuple(zip(*items))\n",
    "\n",
    "grid = Option.product(Option('model', models), \n",
    "                      Option('model_config', [KV(c, alias=k) for k, c in zip(keys, configs)]),\n",
    "                      Option('loss_name', losses))\n",
    "print(list(grid.gen_configs()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference: number of trainable parameters in models:\n",
    "\n",
    "\n",
    "| model | trainable params|\n",
    "|-------|-----------------|\n",
    "|egor_baseline| 1 721 117|\n",
    "|unetatt2_small| 1 747 794|\n",
    "|unetatt3_small| 1 747 794|\n",
    "|unetatt4_small| 1 693 266|\n",
    "|m1_vgg_small| 1 711 395|\n",
    "|m1_resnet_small| 1 691 955|\n",
    "|m3_vgg_small| 1 711 395|\n",
    "|m3_resnet_small| 1 691 955|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "ITERATIONS=5000 # ~ 5 epochs of train in CV\n",
    "TEST_EXECUTE_FREQ=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_stack(x):\n",
    "    return np.expand_dims(np.vstack(x), -1)\n",
    "\n",
    "def make_data(batch, **kwagrs):\n",
    "    return {'trace_raw': exp_stack(batch.raw), 'lift': exp_stack(batch.lift)}\n",
    "\n",
    "def make_data_inference(batch, **kwagrs):\n",
    "    return {'trace_raw': exp_stack(batch.raw)}\n",
    "\n",
    "\n",
    "train_template = (Pipeline()\n",
    "                  .load(components=('raw', 'lift'), fmt='segy', tslice=np.arange(3000))\n",
    "                  .init_model('dynamic', C('model'), name='unet', config=C('model_config'))\n",
    "                  .init_variable('train_loss')\n",
    "                  .train_model('unet', make_data=make_data, fetches=C('loss_name'),\n",
    "                               save_to=V('train_loss'))\n",
    "                  .run_later(BATCH_SIZE, shuffle=True, n_epochs=None)\n",
    "                 )\n",
    "\n",
    "test_template = (Pipeline()\n",
    "                 .init_variable('test_loss')\n",
    "                 .load(components=('raw', 'lift'), fmt='segy', tslice=np.arange(3000))\n",
    "                 .import_model('unet', C('import_from'))\n",
    "                 .predict_model('unet', make_data=make_data, fetches=C('loss_name'),\n",
    "                                save_to=V('test_loss'))\n",
    "                 .run_later(BATCH_SIZE, shuffle=True, n_epochs=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(iteration, experiment, train_ppl, val_set):\n",
    "    \n",
    "    tmp_dump_path = os.path.join(experiment[train_ppl].path, 'tmp')\n",
    "    if os.path.exists(tmp_dump_path):\n",
    "        shutil.rmtree(tmp_dump_path)\n",
    "    os.makedirs(tmp_dump_path)\n",
    "    \n",
    "    inference_pipeline = (val_set.p\n",
    "                         .load(components='raw', fmt='segy', tslice=np.arange(3000))\n",
    "                         .import_model('unet', experiment[train_ppl].pipeline)\n",
    "                         .predict_model('unet', make_data=make_data_inference, fetches=[\"out_lift\"], save_to=B('raw'))\n",
    "                         .dump(path=L(lambda x: os.path.join(tmp_dump_path, str(x) + '.sgy'))(I()), src='raw', fmt='segy', split=False)\n",
    "                         )\n",
    "\n",
    "    inference_pipeline.run(1000, n_epochs=1, drop_last=False, shuffle=False, bar=False)\n",
    "    \n",
    "    output_path = os.path.join(experiment[train_ppl].path, 'res.sgy')\n",
    "    merge_segy_files(output_path=output_path, extra_headers='all', path=os.path.join(tmp_dump_path, '*.sgy'), bar=False)\n",
    "\n",
    "    if os.path.exists(tmp_dump_path):\n",
    "        shutil.rmtree(tmp_dump_path) \n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traces in val set: 160416\n"
     ]
    }
   ],
   "source": [
    "val_index = TraceIndex(index)\n",
    "val_set = SeismicDataset(val_index)\n",
    "print(\"Number of traces in val set:\", len(val_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r 7_research_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research 7_research_res is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 24 jobs with 2 iterations. Totally: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [15:24<00:00, 19.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seismicpro.batchflow.batchflow.research.research.Research at 0x7fa75c38cb00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_name = '7_research_res'\n",
    "\n",
    "val_set = TraceIndex(index)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_template, dataset=train_set, part='train',\n",
    "                          variables='train_loss', name='train_ppl', logging=True)\n",
    "            .add_pipeline(test_template, dataset=train_set, part='test', variables='test_loss', name='test_ppl',\n",
    "                          execute=TEST_EXECUTE_FREQ, run=True, import_from='train_ppl')\n",
    "            .add_function(dump_results, returns='output_path', name='dump_res_fn',\n",
    "                          execute='last', train_ppl='train_ppl', val_set=val_set)\n",
    "            .add_grid(grid)\n",
    "            )\n",
    "\n",
    "research.run(n_iters=ITERATIONS, n_splits=3, shuffle=True, name=res_name, bar=True, gpu=GPUs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = research.load_results(use_alias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      "config         96 non-null object\n",
      "cv_split       96 non-null int64\n",
      "iteration      96 non-null int64\n",
      "name           96 non-null object\n",
      "output_path    24 non-null object\n",
      "repetition     96 non-null int64\n",
      "test_loss      24 non-null float64\n",
      "train_loss     48 non-null float64\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 6.8+ KB\n"
     ]
    }
   ],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_by_config(results, variables, figsize=None, force_flat=True):\n",
    "    if isinstance(variables, dict):\n",
    "        variables = variables.items()\n",
    "    elif len(variables) == 2 and isinstance(variables[0], str):\n",
    "        variables = (variables,)\n",
    "\n",
    "    gbc = results.groupby('config')\n",
    "    n_configs = len(gbc)\n",
    "    n_vars = len(variables)\n",
    "\n",
    "    n_h, n_v = (n_configs, 1) if n_vars == 1 and force_flat else (n_vars, n_configs)\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (n_h * 5, n_v * 5)\n",
    "\n",
    "    _, axs = plt.subplots(n_v, n_h, figsize=figsize)\n",
    "    for x, (config, df) in enumerate(gbc):\n",
    "        for y, (source, val) in enumerate(variables):\n",
    "            if n_configs == 1 and n_vars == 1:\n",
    "                ax = axs\n",
    "            elif n_configs == 1:\n",
    "                ax = axs[y]\n",
    "            elif n_vars == 1:\n",
    "                ax = axs[x]\n",
    "            else:\n",
    "                ax = axs[x, y]\n",
    "            \n",
    "            col = 'cv_split' if 'cv_split' in df.columns else 'repetition'\n",
    "\n",
    "            (df[df['name'] == source]\n",
    "             .pivot(index='iteration', columns=col, values=val)\n",
    "             .rename(columns=lambda s: col + str(s))\n",
    "             .plot(ax=ax))\n",
    "            ax.set_title(config)\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel(val.replace('_', ' ').capitalize())\n",
    "            ax.grid(True)\n",
    "            ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_by_config(res, {'train_ppl': 'train_loss', 'test_ppl': 'test_loss'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = research.load_results()\n",
    "res2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res2[res2.iteration >= 4000]\n",
    " .groupby('model_config')['train_loss', 'test_loss']\n",
    " .agg(['mean', 'std'])\n",
    " .swaplevel(axis=1)\n",
    " .style.background_gradient(axis=None, cmap='cool', subset=['mean'])\n",
    " .background_gradient(axis=None, cmap='cool', subset=['std'])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
